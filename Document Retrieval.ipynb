{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 â€“ Document Retrieval\n",
    "## Written by Xinyao Tian SID: 490085774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are required to complete a program to find similar documents (e.g., articles, queries or questions in sentence) for a given query (e.g., an article or a sentence). The similarity can be defined by you, such as content similarity or emotion similarity such as sentiment. You can use the 20 Newsgroup dataset or any similar dataset you can find.\n",
    "20 Newsgroup Dataset:\n",
    "https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (3 marks) Use tf-idf based document representation for the task. \n",
    "\n",
    "You can use existing\n",
    "third-party libraries for document pre-processing (e.g., tokenization), however, you need to write your own code to derive tf-idf representation and compute similarity between two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/xinyaotian/anaconda3/lib/python3.7/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/xinyaotian/anaconda3/lib/python3.7/site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "# install tokenize function for tokenization\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from math import log\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the limitation of executing power (my old 2018 macbook...)\n",
    "# We only use 7 documents in total to represent the pricple of TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, read all 7 documents into Jupyter notebook\n",
    "# Read the first file\n",
    "text_file = open(\"./sample_files/textScript1.txt\", \"r\")\n",
    "text_content1 = text_file.read()\n",
    "text_file.close()\n",
    "# Read the second file\n",
    "text_file = open(\"./sample_files/textScript2.txt\", \"r\")\n",
    "text_content2 = text_file.read()\n",
    "text_file.close()\n",
    "# Read the second file\n",
    "text_file = open(\"./sample_files/textScript3.txt\", \"r\")\n",
    "text_content3 = text_file.read()\n",
    "text_file.close()\n",
    "# Read the second file\n",
    "text_file = open(\"./sample_files/textScript4.txt\", \"r\")\n",
    "text_content4 = text_file.read()\n",
    "text_file.close()\n",
    "# Read the second file\n",
    "text_file = open(\"./sample_files/textScript5.txt\", \"r\")\n",
    "text_content5 = text_file.read()\n",
    "text_file.close()\n",
    "# Read the second file\n",
    "text_file = open(\"./sample_files/textScript6.txt\", \"r\")\n",
    "text_content6 = text_file.read()\n",
    "text_file.close()\n",
    "# Read the second file\n",
    "text_file = open(\"./sample_files/textScript7.txt\", \"r\")\n",
    "text_content7 = text_file.read()\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Football (or soccer as the game is called in some parts of the world) has a long history. Football in its current form arose in England in the middle of the 19th century. But alternative versions of t\n"
     ]
    }
   ],
   "source": [
    "# check the first 200 characters of the seventh document\n",
    "print(text_content7[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "After import all 7 documents into Jupyter, we first have to split all words by using a thrid-party package called nltk, in order to conduct further steps (like generating weight Matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Calculate Term Frequency\n",
    "def calTermFrequency(text_content):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokenized_list = tknzr.tokenize(text_content)\n",
    "    \n",
    "    # Calculate term frequency\n",
    "    term_frequency_dict = dict()\n",
    "    for term in tokenized_list:\n",
    "        if term in term_frequency_dict.keys():\n",
    "            term_frequency_dict[term] += 1\n",
    "        else:\n",
    "            term_frequency_dict[term] = 1\n",
    "    \n",
    "    # Conduct Log operation for each term\n",
    "    log_tf_dict = dict()\n",
    "    for term in term_frequency_dict:\n",
    "        log_tf_dict[term] = round((1 + log(term_frequency_dict[term], 10)), 4)\n",
    "        \n",
    "    return log_tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all term frequency dict into one huge list\n",
    "def generateLogTermFrequencyList():\n",
    "    # Need further Calsupalate of reading text_content1\n",
    "    log_tf_1 = calTermFrequency(text_content1)\n",
    "    log_tf_2 = calTermFrequency(text_content2)\n",
    "    log_tf_3 = calTermFrequency(text_content3)\n",
    "    log_tf_4 = calTermFrequency(text_content4)\n",
    "    log_tf_5 = calTermFrequency(text_content5)\n",
    "    log_tf_6 = calTermFrequency(text_content6)\n",
    "    log_tf_7 = calTermFrequency(text_content7)\n",
    "    logged_tf_list = list()\n",
    "    \n",
    "    # Put all logged term frequency dict into a huge list to store them\n",
    "    logged_tf_list.append(log_tf_1)\n",
    "    logged_tf_list.append(log_tf_2)\n",
    "    logged_tf_list.append(log_tf_3)\n",
    "    logged_tf_list.append(log_tf_4)\n",
    "    logged_tf_list.append(log_tf_5)\n",
    "    logged_tf_list.append(log_tf_6)\n",
    "    logged_tf_list.append(log_tf_7)\n",
    "    return logged_tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The huge list below\n",
    "logged_tf_list = generateLogTermFrequencyList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'File': 1.0,\n",
       "  '1': 1.0,\n",
       "  ':': 1.0,\n",
       "  'You': 1.0,\n",
       "  'are': 1.0,\n",
       "  'not': 1.0,\n",
       "  'required': 1.0,\n",
       "  'to': 1.0,\n",
       "  'use': 1.0,\n",
       "  'this': 1.0,\n",
       "  'file': 1.301,\n",
       "  '!': 1.301,\n",
       "  'This': 1.4771,\n",
       "  'is': 1.4771,\n",
       "  'an': 1.0,\n",
       "  'txt': 1.0,\n",
       "  'used': 1.0,\n",
       "  'for': 1.0,\n",
       "  'practicing': 1.0,\n",
       "  'Python': 1.0,\n",
       "  '.': 1.699,\n",
       "  'The': 1.301,\n",
       "  'second': 1.0,\n",
       "  'line': 1.4771,\n",
       "  'Hello': 1.0,\n",
       "  ',': 1.0,\n",
       "  'world': 1.0,\n",
       "  '-': 1.6021,\n",
       "  'the': 1.301,\n",
       "  'third': 1.0,\n",
       "  'End': 1.0,\n",
       "  'bottom': 1.0},\n",
       " {'File': 1.0,\n",
       "  '2': 1.0,\n",
       "  ':': 1.0,\n",
       "  'You': 1.301,\n",
       "  'are': 1.0,\n",
       "  'required': 1.0,\n",
       "  'to': 1.301,\n",
       "  'complete': 1.0,\n",
       "  'a': 1.4771,\n",
       "  'program': 1.0,\n",
       "  'find': 1.301,\n",
       "  'similar': 1.301,\n",
       "  'documents': 1.0,\n",
       "  '(': 1.301,\n",
       "  'e': 1.301,\n",
       "  '.': 1.8451,\n",
       "  'g': 1.301,\n",
       "  ',': 1.6021,\n",
       "  'articles': 1.0,\n",
       "  'queries': 1.0,\n",
       "  'or': 1.6021,\n",
       "  'questions': 1.0,\n",
       "  'in': 1.0,\n",
       "  'sentence': 1.301,\n",
       "  ')': 1.301,\n",
       "  'for': 1.0,\n",
       "  'given': 1.0,\n",
       "  'query': 1.0,\n",
       "  'an': 1.0,\n",
       "  'article': 1.0,\n",
       "  'The': 1.0,\n",
       "  'similarity': 1.4771,\n",
       "  'can': 1.4771,\n",
       "  'be': 1.0,\n",
       "  'defined': 1.0,\n",
       "  'by': 1.0,\n",
       "  'you': 1.301,\n",
       "  'such': 1.301,\n",
       "  'as': 1.301,\n",
       "  'content': 1.0,\n",
       "  'emotion': 1.0,\n",
       "  'sentiment': 1.0,\n",
       "  'use': 1.0,\n",
       "  'the': 1.0,\n",
       "  '20': 1.0,\n",
       "  'Newsgroup': 1.0,\n",
       "  'dataset': 1.301,\n",
       "  'any': 1.0},\n",
       " {'An': 1.0,\n",
       "  'athlete': 1.0,\n",
       "  '(': 1.0,\n",
       "  'also': 1.0,\n",
       "  'sportsman': 1.0,\n",
       "  'or': 1.699,\n",
       "  'sportswoman': 1.0,\n",
       "  ')': 1.0,\n",
       "  'is': 1.0,\n",
       "  'a': 1.4771,\n",
       "  'person': 1.0,\n",
       "  'who': 1.0,\n",
       "  'competes': 1.0,\n",
       "  'in': 1.301,\n",
       "  'one': 1.0,\n",
       "  'more': 1.0,\n",
       "  'sports': 1.301,\n",
       "  'that': 1.0,\n",
       "  'involve': 1.0,\n",
       "  'physical': 1.301,\n",
       "  'strength': 1.0,\n",
       "  ',': 1.4771,\n",
       "  'speed': 1.0,\n",
       "  'endurance': 1.0,\n",
       "  '.': 1.6021,\n",
       "  'The': 1.0,\n",
       "  'use': 1.0,\n",
       "  'of': 1.0,\n",
       "  'the': 1.0,\n",
       "  'term': 1.0,\n",
       "  'several': 1.0,\n",
       "  'such': 1.0,\n",
       "  'as': 1.0,\n",
       "  'golf': 1.0,\n",
       "  'auto': 1.0,\n",
       "  'racing': 1.0,\n",
       "  'becomes': 1.0,\n",
       "  'controversial': 1.0,\n",
       "  'issue': 1.0,\n",
       "  'Athletes': 1.0,\n",
       "  'may': 1.0,\n",
       "  'be': 1.0,\n",
       "  'professionals': 1.0,\n",
       "  'amateurs': 1.0,\n",
       "  '[': 1.0,\n",
       "  '1': 1.0,\n",
       "  ']': 1.0,\n",
       "  'Most': 1.0,\n",
       "  'professional': 1.0,\n",
       "  'athletes': 1.0,\n",
       "  'have': 1.0,\n",
       "  'particularly': 1.0,\n",
       "  'well-developed': 1.0,\n",
       "  'physiques': 1.0,\n",
       "  'obtained': 1.0,\n",
       "  'by': 1.301,\n",
       "  'extensive': 1.0,\n",
       "  'training': 1.0,\n",
       "  'and': 1.0,\n",
       "  'strict': 1.301,\n",
       "  'exercise': 1.0,\n",
       "  'accompanied': 1.0,\n",
       "  'dietary': 1.0,\n",
       "  'regimen': 1.0},\n",
       " {'Football': 1.0,\n",
       "  'is': 1.4771,\n",
       "  'a': 1.4771,\n",
       "  'family': 1.0,\n",
       "  'of': 1.4771,\n",
       "  'team': 1.0,\n",
       "  'sports': 1.0,\n",
       "  'that': 1.301,\n",
       "  'involve': 1.0,\n",
       "  ',': 1.4771,\n",
       "  'to': 1.4771,\n",
       "  'varying': 1.301,\n",
       "  'degrees': 1.0,\n",
       "  'kicking': 1.0,\n",
       "  'ball': 1.0,\n",
       "  'score': 1.0,\n",
       "  'goal': 1.0,\n",
       "  '.': 1.6021,\n",
       "  'Unqualified': 1.0,\n",
       "  'the': 1.6021,\n",
       "  'word': 1.301,\n",
       "  'football': 2.0792,\n",
       "  'normally': 1.0,\n",
       "  'means': 1.0,\n",
       "  'form': 1.0,\n",
       "  'most': 1.0,\n",
       "  'popular': 1.0,\n",
       "  'where': 1.0,\n",
       "  'used': 1.0,\n",
       "  'Sports': 1.0,\n",
       "  'commonly': 1.0,\n",
       "  'called': 1.0,\n",
       "  'include': 1.0,\n",
       "  'association': 1.0,\n",
       "  '(': 1.4771,\n",
       "  'known': 1.301,\n",
       "  'as': 1.301,\n",
       "  'soccer': 1.0,\n",
       "  'in': 1.0,\n",
       "  'some': 1.0,\n",
       "  'countries': 1.0,\n",
       "  ');': 1.4771,\n",
       "  'gridiron': 1.0,\n",
       "  'specifically': 1.0,\n",
       "  'American': 1.0,\n",
       "  'or': 1.301,\n",
       "  'Canadian': 1.0,\n",
       "  'Australian': 1.0,\n",
       "  'rules': 1.0,\n",
       "  ';': 1.0,\n",
       "  'rugby': 1.4771,\n",
       "  'either': 1.0,\n",
       "  'union': 1.0,\n",
       "  'league': 1.0,\n",
       "  'and': 1.301,\n",
       "  'Gaelic': 1.0,\n",
       "  '[': 1.301,\n",
       "  '1': 1.0,\n",
       "  ']': 1.301,\n",
       "  '2': 1.0,\n",
       "  'These': 1.0,\n",
       "  'various': 1.0,\n",
       "  'forms': 1.0,\n",
       "  'share': 1.0,\n",
       "  'extent': 1.0,\n",
       "  'common': 1.0,\n",
       "  'origins': 1.0,\n",
       "  'are': 1.0,\n",
       "  'codes': 1.0},\n",
       " {'Although': 1.0,\n",
       "  'the': 1.9031,\n",
       "  'start': 1.0,\n",
       "  'of': 1.699,\n",
       "  'history': 1.0,\n",
       "  'film': 1.0,\n",
       "  'as': 1.301,\n",
       "  'an': 1.0,\n",
       "  'artistic': 1.0,\n",
       "  'medium': 1.0,\n",
       "  'is': 1.0,\n",
       "  'not': 1.0,\n",
       "  'clearly': 1.0,\n",
       "  'defined': 1.0,\n",
       "  ',': 1.699,\n",
       "  'commercial': 1.0,\n",
       "  'public': 1.0,\n",
       "  'screening': 1.0,\n",
       "  'ten': 1.0,\n",
       "  'LumiÃ¨re': 1.301,\n",
       "  'brothers': 1.0,\n",
       "  \"'\": 1.0,\n",
       "  'short': 1.0,\n",
       "  'films': 1.0,\n",
       "  'in': 1.0,\n",
       "  'Paris': 1.0,\n",
       "  'on': 1.0,\n",
       "  '28': 1.0,\n",
       "  'December': 1.0,\n",
       "  '1895': 1.0,\n",
       "  'can': 1.0,\n",
       "  'be': 1.0,\n",
       "  'regarded': 1.0,\n",
       "  'breakthrough': 1.0,\n",
       "  'projected': 1.0,\n",
       "  'cinematographic': 1.301,\n",
       "  'motion': 1.0,\n",
       "  'pictures': 1.0,\n",
       "  '.': 1.301,\n",
       "  'There': 1.0,\n",
       "  'had': 1.0,\n",
       "  'been': 1.0,\n",
       "  'earlier': 1.0,\n",
       "  'results': 1.0,\n",
       "  'and': 1.0,\n",
       "  'screenings': 1.0,\n",
       "  'by': 1.0,\n",
       "  'others': 1.0,\n",
       "  'but': 1.0,\n",
       "  'they': 1.0,\n",
       "  'lacked': 1.0,\n",
       "  'either': 1.0,\n",
       "  'quality': 1.0,\n",
       "  'financial': 1.0,\n",
       "  'backing': 1.0,\n",
       "  'stamina': 1.0,\n",
       "  'or': 1.0,\n",
       "  'luck': 1.0,\n",
       "  'to': 1.0,\n",
       "  'find': 1.0,\n",
       "  'momentum': 1.0,\n",
       "  'that': 1.0,\n",
       "  'propelled': 1.0,\n",
       "  'cinÃ©matographe': 1.0,\n",
       "  'into': 1.0,\n",
       "  'a': 1.0,\n",
       "  'worldwide': 1.0,\n",
       "  'success': 1.0},\n",
       " {'Soon': 1.0,\n",
       "  'film': 1.301,\n",
       "  'production': 1.0,\n",
       "  'companies': 1.0,\n",
       "  'and': 1.6021,\n",
       "  'studios': 1.0,\n",
       "  'were': 1.301,\n",
       "  'established': 1.301,\n",
       "  'all': 1.0,\n",
       "  'over': 1.301,\n",
       "  'the': 1.4771,\n",
       "  'world': 1.0,\n",
       "  '.': 1.6021,\n",
       "  'The': 1.301,\n",
       "  'first': 1.0,\n",
       "  'decade': 1.0,\n",
       "  'of': 1.4771,\n",
       "  'motion': 1.0,\n",
       "  'picture': 1.0,\n",
       "  'saw': 1.0,\n",
       "  'moving': 1.0,\n",
       "  'from': 1.301,\n",
       "  'a': 1.699,\n",
       "  'novelty': 1.0,\n",
       "  'to': 1.0,\n",
       "  'an': 1.0,\n",
       "  'mass': 1.0,\n",
       "  'entertainment': 1.0,\n",
       "  'industry': 1.0,\n",
       "  'earliest': 1.0,\n",
       "  'films': 1.301,\n",
       "  'in': 1.301,\n",
       "  'black': 1.0,\n",
       "  'white': 1.0,\n",
       "  ',': 1.4771,\n",
       "  'under': 1.0,\n",
       "  'minute': 1.0,\n",
       "  'long': 1.0,\n",
       "  'without': 1.0,\n",
       "  'recorded': 1.0,\n",
       "  'sound': 1.0,\n",
       "  'consisted': 1.0,\n",
       "  'single': 1.0,\n",
       "  'shot': 1.0,\n",
       "  'steady': 1.0,\n",
       "  'camera': 1.301,\n",
       "  'Conventions': 1.0,\n",
       "  'toward': 1.0,\n",
       "  'general': 1.0,\n",
       "  'cinematic': 1.301,\n",
       "  'language': 1.0,\n",
       "  'developed': 1.0,\n",
       "  'years': 1.0,\n",
       "  'with': 1.0,\n",
       "  'editing': 1.0,\n",
       "  'movements': 1.0,\n",
       "  'other': 1.0,\n",
       "  'techniques': 1.0,\n",
       "  'contributing': 1.0,\n",
       "  'specific': 1.0,\n",
       "  'roles': 1.0,\n",
       "  'narrative': 1.0},\n",
       " {'Football': 1.301,\n",
       "  '(': 1.0,\n",
       "  'or': 1.0,\n",
       "  'soccer': 1.0,\n",
       "  'as': 1.0,\n",
       "  'the': 2.1761,\n",
       "  'game': 1.699,\n",
       "  'is': 1.0,\n",
       "  'called': 1.301,\n",
       "  'in': 1.699,\n",
       "  'some': 1.301,\n",
       "  'parts': 1.0,\n",
       "  'of': 2.0414,\n",
       "  'world': 1.0,\n",
       "  ')': 1.0,\n",
       "  'has': 1.0,\n",
       "  'a': 1.7782,\n",
       "  'long': 1.0,\n",
       "  'history': 1.4771,\n",
       "  '.': 1.8451,\n",
       "  'its': 1.0,\n",
       "  'current': 1.0,\n",
       "  'form': 1.0,\n",
       "  'arose': 1.0,\n",
       "  'England': 1.0,\n",
       "  'middle': 1.0,\n",
       "  '19th': 1.0,\n",
       "  'century': 1.0,\n",
       "  'But': 1.0,\n",
       "  'alternative': 1.0,\n",
       "  'versions': 1.4771,\n",
       "  'existed': 1.0,\n",
       "  'much': 1.0,\n",
       "  'earlier': 1.0,\n",
       "  'and': 1.4771,\n",
       "  'are': 1.0,\n",
       "  'part': 1.0,\n",
       "  'football': 1.301,\n",
       "  'Early': 1.0,\n",
       "  'precursors': 1.0,\n",
       "  'The': 1.0,\n",
       "  'first': 1.0,\n",
       "  'known': 1.0,\n",
       "  'examples': 1.0,\n",
       "  'team': 1.301,\n",
       "  'involving': 1.0,\n",
       "  'ball': 1.6021,\n",
       "  ',': 1.6021,\n",
       "  'which': 1.0,\n",
       "  'was': 1.4771,\n",
       "  'made': 1.301,\n",
       "  'out': 1.0,\n",
       "  'rock': 1.0,\n",
       "  'occurred': 1.0,\n",
       "  'old': 1.0,\n",
       "  'Mesoamerican': 1.301,\n",
       "  'cultures': 1.0,\n",
       "  'for': 1.0,\n",
       "  'over': 1.301,\n",
       "  '3,000': 1.0,\n",
       "  'years': 1.0,\n",
       "  'ago': 1.0,\n",
       "  'It': 1.0,\n",
       "  'by': 1.0,\n",
       "  'Aztecs': 1.0,\n",
       "  'Tchatali': 1.0,\n",
       "  'although': 1.0,\n",
       "  'various': 1.0,\n",
       "  'were': 1.0,\n",
       "  'spread': 1.0,\n",
       "  'large': 1.0,\n",
       "  'regions': 1.0,\n",
       "  'In': 1.0,\n",
       "  'ritual': 1.0,\n",
       "  'occasions': 1.0,\n",
       "  'would': 1.301,\n",
       "  'symbolize': 1.0,\n",
       "  'sun': 1.0,\n",
       "  'captain': 1.0,\n",
       "  'losing': 1.0,\n",
       "  'be': 1.0,\n",
       "  'sacrificed': 1.0,\n",
       "  'to': 1.301,\n",
       "  'gods': 1.0,\n",
       "  'A': 1.0,\n",
       "  'unique': 1.0,\n",
       "  'feature': 1.0,\n",
       "  'bouncing': 1.0,\n",
       "  'rubber': 1.301,\n",
       "  'â€“': 1.0,\n",
       "  'no': 1.0,\n",
       "  'other': 1.0,\n",
       "  'early': 1.0,\n",
       "  'culture': 1.0,\n",
       "  'had': 1.0,\n",
       "  'access': 1.0}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the list who contains all document term-frequency dict\n",
    "logged_tf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we need a huge list to store all the terms appearing in all documents. Also, we need to make sure that each term is only appear once in this list. We do this by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list that contains all terms appearing among all document\n",
    "def generateOverallTermList(logged_tf_list):\n",
    "    all_term_list = list()\n",
    "    for tf_dict in logged_tf_list:\n",
    "        for term in tf_dict:\n",
    "            if term in all_term_list:\n",
    "                pass\n",
    "            else:\n",
    "                all_term_list.append(term)\n",
    "    \n",
    "    return all_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['File',\n",
       " '1',\n",
       " ':',\n",
       " 'You',\n",
       " 'are',\n",
       " 'not',\n",
       " 'required',\n",
       " 'to',\n",
       " 'use',\n",
       " 'this',\n",
       " 'file',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'txt',\n",
       " 'used',\n",
       " 'for',\n",
       " 'practicing',\n",
       " 'Python',\n",
       " '.',\n",
       " 'The',\n",
       " 'second',\n",
       " 'line',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'world',\n",
       " '-',\n",
       " 'the',\n",
       " 'third',\n",
       " 'End',\n",
       " 'bottom',\n",
       " '2',\n",
       " 'complete',\n",
       " 'a',\n",
       " 'program',\n",
       " 'find',\n",
       " 'similar',\n",
       " 'documents',\n",
       " '(',\n",
       " 'e',\n",
       " 'g',\n",
       " 'articles',\n",
       " 'queries',\n",
       " 'or',\n",
       " 'questions',\n",
       " 'in',\n",
       " 'sentence',\n",
       " ')',\n",
       " 'given',\n",
       " 'query',\n",
       " 'article',\n",
       " 'similarity',\n",
       " 'can',\n",
       " 'be',\n",
       " 'defined',\n",
       " 'by',\n",
       " 'you',\n",
       " 'such',\n",
       " 'as',\n",
       " 'content',\n",
       " 'emotion',\n",
       " 'sentiment',\n",
       " '20',\n",
       " 'Newsgroup',\n",
       " 'dataset',\n",
       " 'any',\n",
       " 'An',\n",
       " 'athlete',\n",
       " 'also',\n",
       " 'sportsman',\n",
       " 'sportswoman',\n",
       " 'person',\n",
       " 'who',\n",
       " 'competes',\n",
       " 'one',\n",
       " 'more',\n",
       " 'sports',\n",
       " 'that',\n",
       " 'involve',\n",
       " 'physical',\n",
       " 'strength',\n",
       " 'speed',\n",
       " 'endurance',\n",
       " 'of',\n",
       " 'term',\n",
       " 'several',\n",
       " 'golf',\n",
       " 'auto',\n",
       " 'racing',\n",
       " 'becomes',\n",
       " 'controversial',\n",
       " 'issue',\n",
       " 'Athletes',\n",
       " 'may',\n",
       " 'professionals',\n",
       " 'amateurs',\n",
       " '[',\n",
       " ']',\n",
       " 'Most',\n",
       " 'professional',\n",
       " 'athletes',\n",
       " 'have',\n",
       " 'particularly',\n",
       " 'well-developed',\n",
       " 'physiques',\n",
       " 'obtained',\n",
       " 'extensive',\n",
       " 'training',\n",
       " 'and',\n",
       " 'strict',\n",
       " 'exercise',\n",
       " 'accompanied',\n",
       " 'dietary',\n",
       " 'regimen',\n",
       " 'Football',\n",
       " 'family',\n",
       " 'team',\n",
       " 'varying',\n",
       " 'degrees',\n",
       " 'kicking',\n",
       " 'ball',\n",
       " 'score',\n",
       " 'goal',\n",
       " 'Unqualified',\n",
       " 'word',\n",
       " 'football',\n",
       " 'normally',\n",
       " 'means',\n",
       " 'form',\n",
       " 'most',\n",
       " 'popular',\n",
       " 'where',\n",
       " 'Sports',\n",
       " 'commonly',\n",
       " 'called',\n",
       " 'include',\n",
       " 'association',\n",
       " 'known',\n",
       " 'soccer',\n",
       " 'some',\n",
       " 'countries',\n",
       " ');',\n",
       " 'gridiron',\n",
       " 'specifically',\n",
       " 'American',\n",
       " 'Canadian',\n",
       " 'Australian',\n",
       " 'rules',\n",
       " ';',\n",
       " 'rugby',\n",
       " 'either',\n",
       " 'union',\n",
       " 'league',\n",
       " 'Gaelic',\n",
       " 'These',\n",
       " 'various',\n",
       " 'forms',\n",
       " 'share',\n",
       " 'extent',\n",
       " 'common',\n",
       " 'origins',\n",
       " 'codes',\n",
       " 'Although',\n",
       " 'start',\n",
       " 'history',\n",
       " 'film',\n",
       " 'artistic',\n",
       " 'medium',\n",
       " 'clearly',\n",
       " 'commercial',\n",
       " 'public',\n",
       " 'screening',\n",
       " 'ten',\n",
       " 'LumiÃ¨re',\n",
       " 'brothers',\n",
       " \"'\",\n",
       " 'short',\n",
       " 'films',\n",
       " 'Paris',\n",
       " 'on',\n",
       " '28',\n",
       " 'December',\n",
       " '1895',\n",
       " 'regarded',\n",
       " 'breakthrough',\n",
       " 'projected',\n",
       " 'cinematographic',\n",
       " 'motion',\n",
       " 'pictures',\n",
       " 'There',\n",
       " 'had',\n",
       " 'been',\n",
       " 'earlier',\n",
       " 'results',\n",
       " 'screenings',\n",
       " 'others',\n",
       " 'but',\n",
       " 'they',\n",
       " 'lacked',\n",
       " 'quality',\n",
       " 'financial',\n",
       " 'backing',\n",
       " 'stamina',\n",
       " 'luck',\n",
       " 'momentum',\n",
       " 'propelled',\n",
       " 'cinÃ©matographe',\n",
       " 'into',\n",
       " 'worldwide',\n",
       " 'success',\n",
       " 'Soon',\n",
       " 'production',\n",
       " 'companies',\n",
       " 'studios',\n",
       " 'were',\n",
       " 'established',\n",
       " 'all',\n",
       " 'over',\n",
       " 'first',\n",
       " 'decade',\n",
       " 'picture',\n",
       " 'saw',\n",
       " 'moving',\n",
       " 'from',\n",
       " 'novelty',\n",
       " 'mass',\n",
       " 'entertainment',\n",
       " 'industry',\n",
       " 'earliest',\n",
       " 'black',\n",
       " 'white',\n",
       " 'under',\n",
       " 'minute',\n",
       " 'long',\n",
       " 'without',\n",
       " 'recorded',\n",
       " 'sound',\n",
       " 'consisted',\n",
       " 'single',\n",
       " 'shot',\n",
       " 'steady',\n",
       " 'camera',\n",
       " 'Conventions',\n",
       " 'toward',\n",
       " 'general',\n",
       " 'cinematic',\n",
       " 'language',\n",
       " 'developed',\n",
       " 'years',\n",
       " 'with',\n",
       " 'editing',\n",
       " 'movements',\n",
       " 'other',\n",
       " 'techniques',\n",
       " 'contributing',\n",
       " 'specific',\n",
       " 'roles',\n",
       " 'narrative',\n",
       " 'game',\n",
       " 'parts',\n",
       " 'has',\n",
       " 'its',\n",
       " 'current',\n",
       " 'arose',\n",
       " 'England',\n",
       " 'middle',\n",
       " '19th',\n",
       " 'century',\n",
       " 'But',\n",
       " 'alternative',\n",
       " 'versions',\n",
       " 'existed',\n",
       " 'much',\n",
       " 'part',\n",
       " 'Early',\n",
       " 'precursors',\n",
       " 'examples',\n",
       " 'involving',\n",
       " 'which',\n",
       " 'was',\n",
       " 'made',\n",
       " 'out',\n",
       " 'rock',\n",
       " 'occurred',\n",
       " 'old',\n",
       " 'Mesoamerican',\n",
       " 'cultures',\n",
       " '3,000',\n",
       " 'ago',\n",
       " 'It',\n",
       " 'Aztecs',\n",
       " 'Tchatali',\n",
       " 'although',\n",
       " 'spread',\n",
       " 'large',\n",
       " 'regions',\n",
       " 'In',\n",
       " 'ritual',\n",
       " 'occasions',\n",
       " 'would',\n",
       " 'symbolize',\n",
       " 'sun',\n",
       " 'captain',\n",
       " 'losing',\n",
       " 'sacrificed',\n",
       " 'gods',\n",
       " 'A',\n",
       " 'unique',\n",
       " 'feature',\n",
       " 'bouncing',\n",
       " 'rubber',\n",
       " 'â€“',\n",
       " 'no',\n",
       " 'early',\n",
       " 'culture',\n",
       " 'access']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list with all terms who appear in all documents\n",
    "all_term_list = generateOverallTermList(logged_tf_list)\n",
    "all_term_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a binary Matrix\n",
    "We generate a matrix only containing 0 or 1 to refer whether a term occurs in a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Binary Matrix which represents if a term occurs in a specific document\n",
    "def generateBinMatrix(logged_tf_list, all_term_list):\n",
    "    # Initiate a empty dictionary in order to save a Word Vector of each document\n",
    "    bin_matrix_dict = dict()\n",
    "    count = 0\n",
    "    # For every document which exists in the record list\n",
    "    for file_dict in logged_tf_list:\n",
    "        word_exist_list = list()\n",
    "        # Nominate the document name\n",
    "        filename = \"Filename_\" + str(count)\n",
    "        count += 1\n",
    "        # For each word in the overall word list, \n",
    "        for word in all_term_list:\n",
    "            # if it exsits in this document, then set the value to 1\n",
    "            if word in file_dict:\n",
    "                word_exist_list.append(1)\n",
    "            # otherwise, set the value to 0, indicating that this word doesn't appear in this Doc\n",
    "            else:\n",
    "                word_exist_list.append(0)\n",
    "        \n",
    "        # Add the Word Vector of this document to the general dict\n",
    "        # In order to form a BIG 2x2 Table\n",
    "        bin_matrix_dict[filename] = word_exist_list\n",
    "    \n",
    "    return bin_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually execute the function\n",
    "bin_dict = generateBinMatrix(logged_tf_list, all_term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename_0</th>\n",
       "      <th>Filename_1</th>\n",
       "      <th>Filename_2</th>\n",
       "      <th>Filename_3</th>\n",
       "      <th>Filename_4</th>\n",
       "      <th>Filename_5</th>\n",
       "      <th>Filename_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename_0  Filename_1  Filename_2  Filename_3  Filename_4  Filename_5  \\\n",
       "0            1           1           0           0           0           0   \n",
       "1            1           0           1           1           0           0   \n",
       "2            1           1           0           0           0           0   \n",
       "3            1           1           0           0           0           0   \n",
       "4            1           1           0           1           0           0   \n",
       "5            1           0           0           0           1           0   \n",
       "6            1           1           0           0           0           0   \n",
       "7            1           1           0           1           1           1   \n",
       "8            1           1           1           0           0           0   \n",
       "9            1           0           0           0           0           0   \n",
       "10           1           0           0           0           0           0   \n",
       "11           1           0           0           0           0           0   \n",
       "\n",
       "    Filename_6  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            1  \n",
       "5            0  \n",
       "6            0  \n",
       "7            1  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  \n",
       "11           0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could see the first several lices by this binary matrix\n",
    "# each column name represents a file, and each row index represents a specific word occuring in the huge list\n",
    "binMatrix_df = pd.DataFrame(bin_dict)\n",
    "# Print the shape of this binary matrix\n",
    "print(binMatrix_df.shape)\n",
    "binMatrix_df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate inversed document-frequency\n",
    "Since we already have this binary matrix, we could now calculate the idf by this function:\n",
    "The function of idf: log10(N/dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function of calculating tf-idf\n",
    "def calcualetIdf(binMatrix_df):\n",
    "    # Get the total number of columns\n",
    "    total_number_of_documents = int(binMatrix_df.shape[1])\n",
    "    # Get the occurance times of each word\n",
    "    docFrequency_series = binMatrix_df.sum(axis=1)\n",
    "    # Init a list storing idf \n",
    "    idf_list = list()\n",
    "    # Iterate words in order to calculate idf\n",
    "    for _, frequency in docFrequency_series.iteritems():\n",
    "        # The function of idf: log10(N/dft)\n",
    "        # In order to save memory and speed up, we only maintain 4 decimal places by round()\n",
    "        idf = round(log((total_number_of_documents * (1/frequency)), 10), 4)\n",
    "        idf_list.append(idf)\n",
    "    return idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5441, 0.368, 0.5441, 0.5441, 0.243, 0.5441, 0.5441, 0.0669, 0.368, 0.8451]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the function above\n",
    "idf_list = calcualetIdf(binMatrix_df)\n",
    "print(len(idf_list))\n",
    "# See idf of each terms (the first 10)\n",
    "idf_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generate a dict to store the match between a term and its idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a term-idf matching dict\n",
    "def generateIdfDict(idf_list, all_term_list):\n",
    "    idf_word_dict = dict()\n",
    "    i = 0\n",
    "    while i < len(all_term_list):\n",
    "        term = all_term_list[i]\n",
    "        idf_word_dict[term] = idf_list[i]\n",
    "        i += 1\n",
    "    return idf_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_word_dict = generateIdfDict(idf_list, all_term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF\n",
    "Now, both term-frequency of each document and idf of each term are hold in our hands,\n",
    "thus we could calculate tf-idf eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Tf-idf\n",
    "def calculateWeightMatrix(idf_word_dict, logged_tf_list, all_term_list):\n",
    "    overall_list = list()\n",
    "    count = 0\n",
    "    # match the term in overall-term with each Doc\n",
    "    for doc in logged_tf_list:\n",
    "        tf_idf_list = list()\n",
    "        for term in all_term_list:\n",
    "            if term in doc.keys():\n",
    "                tfIdf = round( (idf_word_dict[term] * doc[term]), 4)\n",
    "                tf_idf_list.append(tfIdf)\n",
    "            else:\n",
    "                tf_idf_list.append(0.0)\n",
    "        \n",
    "        # overall_list.append({str(count):tf_idf_list})\n",
    "        overall_list.append(tf_idf_list)\n",
    "        count += 1\n",
    "    return overall_list\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>File</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.7079</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>required</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>1.0995</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>1.0995</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This</th>\n",
       "      <td>1.2483</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2158</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.2158</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>practicing</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line</th>\n",
       "      <td>1.2483</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>1.3539</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cultures</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,000</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ago</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aztecs</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tchatali</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>although</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spread</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regions</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ritual</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occasions</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbolize</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>captain</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losing</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sacrificed</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gods</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bouncing</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rubber</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â€“</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0       1       2       3       4       5       6\n",
       "File        0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "1           0.3680  0.0000  0.3680  0.3680  0.0000  0.0000  0.0000\n",
       ":           0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "You         0.5441  0.7079  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "are         0.2430  0.2430  0.0000  0.2430  0.0000  0.0000  0.2430\n",
       "not         0.5441  0.0000  0.0000  0.0000  0.5441  0.0000  0.0000\n",
       "required    0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "to          0.0669  0.0870  0.0000  0.0988  0.0669  0.0669  0.0870\n",
       "use         0.3680  0.3680  0.3680  0.0000  0.0000  0.0000  0.0000\n",
       "this        0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "file        1.0995  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "!           1.0995  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "This        1.2483  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "is          0.2158  0.0000  0.1461  0.2158  0.1461  0.0000  0.1461\n",
       "an          0.2430  0.2430  0.0000  0.0000  0.2430  0.2430  0.0000\n",
       "txt         0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "used        0.5441  0.0000  0.0000  0.5441  0.0000  0.0000  0.0000\n",
       "for         0.3680  0.3680  0.0000  0.0000  0.0000  0.0000  0.3680\n",
       "practicing  0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "Python      0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       ".           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "The         0.1901  0.1461  0.1461  0.0000  0.0000  0.1901  0.1461\n",
       "second      0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "line        1.2483  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "Hello       0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       ",           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "world       0.3680  0.0000  0.0000  0.0000  0.0000  0.3680  0.3680\n",
       "-           1.3539  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "the         0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "third       0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "...            ...     ...     ...     ...     ...     ...     ...\n",
       "cultures    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "3,000       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "ago         0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "It          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "Aztecs      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "Tchatali    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "although    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "spread      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "large       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "regions     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "In          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "ritual      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "occasions   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "would       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0995\n",
       "symbolize   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "sun         0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "captain     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "losing      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "sacrificed  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "gods        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "A           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "unique      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "feature     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "bouncing    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "rubber      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.0995\n",
       "â€“           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "no          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "early       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "culture     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "access      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "\n",
       "[317 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we use a Pandas DataFrame to demostrate the weight Matrix\n",
    "weight_matrix = calculateWeightMatrix(idf_word_dict, logged_tf_list, all_term_list)\n",
    "weight_matrix_df = pd.DataFrame(weight_matrix).transpose()\n",
    "weight_matrix_df.index = all_term_list\n",
    "weight_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Similarity between two document\n",
    "Here, we will use consine similarity to compute how similar each two documents are.\n",
    "Before we do that, first we need to write some mathmatical function for computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate norm\n",
    "def calculateNorm(term_vector):\n",
    "    sum_of_value = 0\n",
    "    for _, value in term_vector.iteritems():\n",
    "        sum_of_value += value **2\n",
    "    return round(sqrt(sum_of_value), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weight_matrix_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate dotProduct\n",
    "def calculateDotProduct(term_vector1, term_vector2):\n",
    "    dotProduct = np.dot(term_vector1.values, term_vector2.values)\n",
    "    return round(dotProduct, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine similarity\n",
    "def computeCosSimilarity(term_vector1, term_vector2):\n",
    "    norm0 = calculateNorm(term_vector1)\n",
    "    norm1 = calculateNorm(term_vector2)\n",
    "    dotProduct = calculateDotProduct(term_vector1, term_vector2)\n",
    "    cosine_value = dotProduct / ((norm0) * (norm1))\n",
    "    return round(cosine_value, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could compute similarity between each two documents. The value should between [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.085471"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the first two\n",
    "cosSimilarity1 = computeCosSimilarity(weight_matrix_df[0], weight_matrix_df[1])\n",
    "cosSimilarity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03664"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see another\n",
    "cosSimilarity2 = computeCosSimilarity(weight_matrix_df[2], weight_matrix_df[1])\n",
    "cosSimilarity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we calculate the similarity of the same document?\n",
    "cosSimilarity3 = computeCosSimilarity(weight_matrix_df[6], weight_matrix_df[6])\n",
    "cosSimilarity3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's 1.0, meaning they are totally same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (2 marks) Improve the tf-idf based method or design a better method for improved retrieval performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in our slide (Wk02, Page23), there are many variants of tf-idf.\n",
    "\n",
    "We choose augmented term-frequency as our new term-frequency.\n",
    "\n",
    "Due to the boolean properity, it's extremely suitable for speed up calculation on a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, improve the tf-idf by augmented term-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Augmented Tf-idf\n",
    "def calculateAugmentedWeightMatrix(idf_word_dict, logged_tf_list, all_term_list):\n",
    "    overall_list = list()\n",
    "    count = 0\n",
    "    # match the term in overall-term with each Doc\n",
    "    for doc in logged_tf_list:\n",
    "        tf_idf_list = list()\n",
    "        for term in all_term_list:\n",
    "            if term in doc.keys():\n",
    "                # Modify this place, change the term-frequency to augmented term-frequency\n",
    "                # tfIdf = round( (idf_word_dict[term] * doc[term]), 4)\n",
    "                tfIdf = round( ( idf_word_dict[term] * 1), 4)\n",
    "                tf_idf_list.append(tfIdf)\n",
    "            else:\n",
    "                tf_idf_list.append(0.0)\n",
    "        \n",
    "        # overall_list.append({str(count):tf_idf_list})\n",
    "        overall_list.append(tf_idf_list)\n",
    "        count += 1\n",
    "    return overall_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code by your hands, you will defintely find it's really faster than the previous one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>File</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>required</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>practicing</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>0.1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cultures</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,000</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ago</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aztecs</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tchatali</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>although</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spread</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regions</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ritual</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occasions</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbolize</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>captain</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losing</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sacrificed</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gods</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bouncing</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rubber</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â€“</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0       1       2       3       4       5       6\n",
       "File        0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "1           0.3680  0.0000  0.3680  0.3680  0.0000  0.0000  0.0000\n",
       ":           0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "You         0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "are         0.2430  0.2430  0.0000  0.2430  0.0000  0.0000  0.2430\n",
       "not         0.5441  0.0000  0.0000  0.0000  0.5441  0.0000  0.0000\n",
       "required    0.5441  0.5441  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "to          0.0669  0.0669  0.0000  0.0669  0.0669  0.0669  0.0669\n",
       "use         0.3680  0.3680  0.3680  0.0000  0.0000  0.0000  0.0000\n",
       "this        0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "file        0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "!           0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "This        0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "is          0.1461  0.0000  0.1461  0.1461  0.1461  0.0000  0.1461\n",
       "an          0.2430  0.2430  0.0000  0.0000  0.2430  0.2430  0.0000\n",
       "txt         0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "used        0.5441  0.0000  0.0000  0.5441  0.0000  0.0000  0.0000\n",
       "for         0.3680  0.3680  0.0000  0.0000  0.0000  0.0000  0.3680\n",
       "practicing  0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "Python      0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       ".           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "The         0.1461  0.1461  0.1461  0.0000  0.0000  0.1461  0.1461\n",
       "second      0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "line        0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "Hello       0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       ",           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "world       0.3680  0.0000  0.0000  0.0000  0.0000  0.3680  0.3680\n",
       "-           0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "the         0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "third       0.8451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "...            ...     ...     ...     ...     ...     ...     ...\n",
       "cultures    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "3,000       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "ago         0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "It          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "Aztecs      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "Tchatali    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "although    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "spread      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "large       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "regions     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "In          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "ritual      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "occasions   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "would       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "symbolize   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "sun         0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "captain     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "losing      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "sacrificed  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "gods        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "A           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "unique      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "feature     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "bouncing    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "rubber      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "â€“           0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "no          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "early       0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "culture     0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "access      0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8451\n",
       "\n",
       "[317 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we use a Pandas DataFrame to demostrate the augmented weight Matrix\n",
    "\n",
    "aug_weight_matrix = calculateAugmentedWeightMatrix(idf_word_dict, logged_tf_list, all_term_list)\n",
    "aug_weight_matrix_df = pd.DataFrame(aug_weight_matrix).transpose()\n",
    "aug_weight_matrix_df.index = all_term_list\n",
    "aug_weight_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, calculate the centroid of each numbers of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always useful to have a centroid between two sets of documents.\n",
    "\n",
    "But since we don't have the functionality of what documents are related, we could demonstrate the implementation by two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate centroid\n",
    "def calculateCentroid(term_vector1, term_vector2):\n",
    "    centroid = (term_vector1.values + term_vector2.values)/2\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you will get a centroid vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5441 , 0.184  , 0.5441 , 0.5441 , 0.243  , 0.27205, 0.5441 ,\n",
       "       0.0669 , 0.368  , 0.42255, 0.42255, 0.42255, 0.42255, 0.07305,\n",
       "       0.243  , 0.42255, 0.27205, 0.368  , 0.42255, 0.42255, 0.     ,\n",
       "       0.1461 , 0.42255, 0.42255, 0.42255, 0.     , 0.184  , 0.42255,\n",
       "       0.     , 0.42255, 0.42255, 0.42255, 0.27205, 0.42255, 0.03345,\n",
       "       0.42255, 0.27205, 0.42255, 0.42255, 0.1215 , 0.42255, 0.42255,\n",
       "       0.42255, 0.42255, 0.07305, 0.42255, 0.03345, 0.42255, 0.184  ,\n",
       "       0.42255, 0.42255, 0.42255, 0.42255, 0.27205, 0.1215 , 0.27205,\n",
       "       0.1215 , 0.42255, 0.27205, 0.07305, 0.42255, 0.42255, 0.42255,\n",
       "       0.42255, 0.42255, 0.42255, 0.42255, 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the centorid vector betwween document 0 and 1\n",
    "centroid_01 = calculateCentroid(aug_weight_matrix_df[0], aug_weight_matrix_df[1])\n",
    "centroid_01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's the end of my homework, hope you enjoy it! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
